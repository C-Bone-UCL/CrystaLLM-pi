{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237ed6c1",
   "metadata": {},
   "source": [
    "### Full worklflow of employing CrystaLLM-pi for materials discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d657a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import __init__ # this just ensures were in the right env\n",
    "from pymatgen.io.jarvis import JarvisAtomsAdaptor as JAA\n",
    "from jarvis.core.atoms import Atoms\n",
    "from pymatgen.io.cif import CifWriter\n",
    "import json\n",
    "from pymatgen.symmetry.analyzer import SpacegroupAnalyzer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b0e2b0",
   "metadata": {},
   "source": [
    "### Step 1. Make the dataset in an apropriate format\n",
    "- Data source\n",
    "  - 1 json file that contains materials project material ids and their associated pmg structure objects\n",
    "  - 1 json file that has the material ids and associated SLMEs\n",
    "  > In the field of photovoltaics (PVs), SLME stands for Spectroscopic Limited Maximum Efficiency. It's a theoretical metric used to predict the maximum possible energy conversion efficiency of a solar cell material. Its more detailed and material-specific than the widely used Shockley-Queisser limit. See [this paper](https://arxiv.org/pdf/2507.13246) for more details on ML and SLMEs\n",
    "\n",
    "- Columns we want to finetune CrystaLLM-pi\n",
    "  - We want to condition generation on SLME, to try and find novel materials with high SLMEs\n",
    "  - 'Material ID': unique identifier, here for traceability of training data\n",
    "  - 'Reduced Formula': This will be used to speed up novelty metric calculations\n",
    "  - 'CIF': text sequences that the language model will train on\n",
    "  - 'SLME': the property that the model will condition its sequence generation on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7cbbf6",
   "metadata": {},
   "source": [
    "#### Lets build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d48d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _symmetrize_cif(struct):\n",
    "    \"\"\"Convert structure to symmetrized CIF format with error handling.\"\"\"\n",
    "    sga = SpacegroupAnalyzer(struct)\n",
    "    symm_struct = sga.get_symmetrized_structure()\n",
    "    return str(CifWriter(symm_struct, symprec=0.1))\n",
    "def structure_to_cif(struct):\n",
    "    \"\"\"Convert a pymatgen Structure to CIF format w symmetry\"\"\"\n",
    "    cif_str = _symmetrize_cif(struct)\n",
    "    return cif_str\n",
    "def extract_formula(struct):\n",
    "    \"\"\"Extract the reduced formula from a pymatgen Structure.\"\"\"\n",
    "    return struct.composition.reduced_formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d7ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HF-databases/mpdb-slme/all_structures.json','r') as f:\n",
    "    structure_dict = json.load(f)\n",
    "with open('HF-databases/mpdb-slme/All_corr_SLMEs.json','r') as f:\n",
    "    slme_dict = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(columns=['Material ID', 'Reduced Formula', 'CIF'])\n",
    "for material_id, struct_dict in tqdm(structure_dict.items()):\n",
    "    ats = Atoms.from_dict(struct_dict)\n",
    "    structure = JAA.get_structure(ats)\n",
    "    cif_str = structure_to_cif(structure)\n",
    "    formula = extract_formula(structure)\n",
    "    df = pd.concat([df, pd.DataFrame({'Material ID': [material_id], 'Reduced Formula': [formula], 'CIF': [cif_str]})], ignore_index=True)\n",
    "    df['SLME'] = df['Material ID'].map(slme_dict)\n",
    "\n",
    "df.to_parquet('HF-databases/mpdb-slme/mpdb-slme.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb00e185",
   "metadata": {},
   "source": [
    "#### We can visualise the distribution of SLMEs in the finetuning set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4f35c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['SLME'].dropna(), bins=50)\n",
    "plt.xlabel('SLME')Colla\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of SLME values in MPDB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060fdedb",
   "metadata": {},
   "source": [
    "#### Clean and augment the CIFs\n",
    "- We turn the CIFs from standard into the correct format for CrystaLLM-pi training (process is invertible)\n",
    "- We normalise the property we train on to stabilise training\n",
    "- We also filtered so no augmented CIFs exceed context length (but none did so can skip)\n",
    "> Note: We don't deduplicate because the dataset is already curated and each structure has different SLMEs so they are of interest for training. If you want to do this, see the [README.md](README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241958f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python _utils/_preprocessing/_cleaning.py \\\n",
    "    --input_parquet HF-databases/mpdb-slme/mpdb-slme.parquet \\\n",
    "    --output_parquet HF-databases/mpdb-slme/mpdb-slme_clean_filtered.parquet \\\n",
    "    --property_columns \"['SLME']\" \\\n",
    "    --property1_normaliser \"linear\" \\\n",
    "    --num_workers 8 \\\n",
    "    --filter_to 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058d96a",
   "metadata": {},
   "source": [
    "### Save to HF\n",
    "- Because we have a small dataset, we would like to train on all of it\n",
    "- Datasets available on HuggingFace:\n",
    "  - c-bone/mpdb-slme-full (100% - train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763cf6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python _utils/_preprocessing/_save_dataset_to_HF.py \\\n",
    "    --input_parquet 'HF-databases/mpdb-slme/mpdb-slme_clean_filtered.parquet' \\\n",
    "    --output_parquet 'HF-databases/mpdb-slme/mpdb-slme-full.parquet' \\\n",
    "    --valid_size 0.00 \\\n",
    "    --test_size 0.00 \\\n",
    "    --save_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2f2b82",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "- I performed a quick hyperparameter search using a the same dataset but with a small validation set, best params stored in the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b1ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=2 _train.py --config '_config_files/training/conditional/ft-slme/slme_ft-PKV-opt.jsonc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e64004",
   "metadata": {},
   "source": [
    "#### Make Prompts\n",
    "- Here the prompts are quite simple, we target any composition but the highest possible SLME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d681875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python _utils/_generating/make_prompts.py \\\n",
    "    --manual \\\n",
    "    --compositions \"\" \\\n",
    "    --condition_lists \"1.0\" \\\n",
    "    --output_parquet '_utils/_evaluation_files/conditional_studies/slme/slme-PKV-opt_prompt.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf47f87",
   "metadata": {},
   "source": [
    "#### Materials Generation\n",
    "- With default params that balance exploration and exploitation of learned patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5577ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python _utils/_generating/generate_CIFs.py --config '_config_files/generation/conditional/slme/slme-PKV-opt_eval.jsonc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac29d0",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "- We want to flag materials that are valid, unique, novel, and predicted stable for further analysis\n",
    "- We can screen this pretty fast with the metrics scripts set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d94b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python _utils/_metrics/VUN_metrics.py \\\n",
    "    --input_parquet '_utils/_evaluation_files/conditional_studies/slme/slme-PKV-opt_gen.parquet' \\\n",
    "    --huggingface_dataset 'c-bone/mpdb-slme-full' \\\n",
    "    --load_processed_data 'HF-databases/mpdb-slme/mpdb-slme-full_proc.parquet' \\\n",
    "    --output_parquet '_utils/_evaluation_files/conditional_studies/slme/slme-PKV-opt_post.parquet' \\\n",
    "    --num_workers 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f12866",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python _utils/_metrics/mace_ehull.py \\\n",
    "    --post_parquet '_utils/_evaluation_files/conditional_studies/slme/slme-PKV-opt_post.parquet' \\\n",
    "    --output_parquet '_utils/_evaluation_files/conditional_studies/slme/slme-PKV-opt_post-s.parquet' \\\n",
    "    --mp_data 'mp_computed_structure_entries.json.gz' \\\n",
    "    --num_workers 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26477f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import __init__\n",
    "import pandas as pd\n",
    "df = pd.read_parquet('_artifacts/slme/slme-PKV-opt_post-s.parquet')\n",
    "\n",
    "valid_count = df['is_valid'].sum()\n",
    "valid_unique_count = df[(df['is_valid']) & (df['is_unique'])].shape[0]\n",
    "valid_unique_novel_count = df[(df['is_valid']) & (df['is_unique']) & (df['is_novel'])].shape[0]\n",
    "low_ehull_count = df[(df['is_valid']) & (df['is_unique']) & (df['is_novel']) & (df['ehull_mace_mp'] < 0.1)].shape[0]\n",
    "print(f\"Valid entries: {valid_count}\")\n",
    "print(f\"Valid and unique entries: {valid_unique_count}\")\n",
    "print(f\"Valid, unique, and novel entries: {valid_unique_novel_count}\")\n",
    "print(f\"Valid, unique, novel entries with ehull_mace_mp < 0.1: {low_ehull_count}\")\n",
    "print(f\"Total entries: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3c115",
   "metadata": {},
   "source": [
    "#### Surrogate Model Screening\n",
    "- The valid, unique, novel entries were screened using an ALIGNN model trained on Hybrid bandgaps to look at the output materials\n",
    "- They were also screened using an ALIGNN model that was trained on SLME data\n",
    "\n",
    "- So for each of the VSUN materials above, we calculated a series of metrics (Surrogate BG, Surrogate SLME, Sustainability scores (via HHI scores)), then mapped them back to the structures in `_artifacts/slme/slme-PKV-opt_post-s.parquet'\n",
    "  - How to get surrogate metrics are not included here (calculations performed by someone else in the group), but the HHI score is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import __init__\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff5217",
   "metadata": {},
   "source": [
    "1. We get the Surrogate SLME and band-gap, and map structs to post-s df (material_id in the 'gen_merged' is the index of the post-s df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3217cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('_artifacts/slme/slme-PKV-opt_post-s.parquet')\n",
    "gen_merged = pd.read_parquet('_artifacts/slme/slme-PKV-opt_post-surrogate-metrics.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9934ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b395f",
   "metadata": {},
   "source": [
    "2. Add sustainability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b32085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _utils import get_hhi_scores_from_cif\n",
    "# apply to gen_merged on 'Generated CIF' column\n",
    "gen_merged[['HHI_p', 'HHI_r']] = gen_merged['Generated CIF'].apply(\n",
    "    lambda cif: pd.Series(get_hhi_scores_from_cif(cif))\n",
    ")\n",
    "gen_merged['HHI_distance_to_0'] = ((gen_merged['HHI_p']**2 + gen_merged['HHI_r']**2)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_merged.to_parquet('_artifacts/slme/slme-PKV-opt_post-surrogate-metrics-hhi.parquet', index=False)\n",
    "gen_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab2a929",
   "metadata": {},
   "source": [
    "3. We have already filtered down to VSUN (using the FT set as reference, and calculating structural novelty. But here want to look at further metrics, so lets use the gen_merged df, and get novelt metrics when we do both structural and compositional novelty, as well as using fine-tune or pre-training dataset as the reference dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2fce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python _utils/_metrics/VUN_metrics.py \\\n",
    "    --input_parquet '_artifacts/slme/slme-PKV-opt_post-surrogate-metrics-hhi.parquet' \\\n",
    "    --huggingface_dataset 'c-bone/mpdb-slme-full' \\\n",
    "    --load_processed_data 'HF-databases/mpdb-slme/mpdb-slme-full_proc.parquet' \\\n",
    "    --output_parquet '_artifacts/slme/slme-PKV-opt_post-surrogate-metrics-hhi-ft-set.parquet' \\\n",
    "    --num_workers 32 \\\n",
    "    --check_comp_novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd965ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V+U+Novel: 16463 \n",
    "# V+U+CompNovel: 15462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b63fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python _utils/_metrics/VUN_metrics.py \\\n",
    "    --input_parquet '_artifacts/slme/slme-PKV-opt_post-surrogate-metrics-hhi.parquet' \\\n",
    "    --huggingface_dataset 'c-bone/lematerial_clean' \\\n",
    "    --load_processed_data 'HF-databases/lematerial/lematerial_dedup.parquet' \\\n",
    "    --output_parquet '_artifacts/slme/slme-PKV-opt_post-surrogate-metrics-pt-set.parquet' \\\n",
    "    --num_workers 32 \\\n",
    "    --check_comp_novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V+U+Novel: 8712\n",
    "# V+U+CompNovel: 1809"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import __init__\n",
    "import pandas as pd\n",
    "\n",
    "df_ft = pd.read_parquet('_artifacts/slme/slme-PKV-opt_post-surrogate-metrics-hhi-ft-set.parquet')\n",
    "df_pt = pd.read_parquet('_artifacts/slme/slme-PKV-opt_post-surrogate-metrics-hhi-pt-set.parquet')\n",
    "\n",
    "# in df_pt, rename is_novel to is_novel_pt, and is_comp_novel to is_comp_novel_pt\n",
    "df_pt = df_pt.rename(columns={'is_novel': 'is_novel_pt', 'is_comp_novel': 'is_comp_novel_pt'})\n",
    "\n",
    "# merge df and df_pt on material_id\n",
    "df_merged = df_ft.merge(df_pt[['material_id', 'is_novel_pt', 'is_comp_novel_pt']], on='material_id', how='left')\n",
    "\n",
    "df_merged.to_parquet('_artifacts/slme/slme-PKV-opt_post-all-metrics', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cd5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b6a8f",
   "metadata": {},
   "source": [
    "4. Now we save the candidates to CIFs in a directory. We give name them according to reduced formula, scoring (SLME is just highestr pred SLME, Sustain-SLME is an SLME > 25 ranked by HHI_distance_to_0), what their position is in ranking wrt metric, and what type of novelty they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb9307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import __init__\n",
    "from _utils import run_material_selection\n",
    "\n",
    "out, out = run_material_selection(\n",
    "    input_parquet='_artifacts/slme/slme-PKV-opt_post-all-metrics',\n",
    "    output_dir='SLME_candidates_test',\n",
    "    output_csv='SLME_candidates_test.csv',\n",
    "    top_n_slme=15,\n",
    "    top_n_sustain=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd738989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import __init__\n",
    "\n",
    "!python _utils/_metrics/dft_ehull.py \\\n",
    "    --input_csv '_artifacts/slme/cyprien-dft-slme.csv' \\\n",
    "    --output_parquet '_artifacts/slme/cyprien-dft-slme-ehull.parquet' \\\n",
    "    --output_cif_dir '_artifacts/slme/cyprien-dft-slme-cifs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b0f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('_artifacts/slme/cyprien-dft-slme-ehull.parquet')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystallmv2_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

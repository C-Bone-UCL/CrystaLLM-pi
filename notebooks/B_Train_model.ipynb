{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigated to package root: /home/cyprien/CrystaLLMv2_PKV\n",
      "Added package root to Python path\n"
     ]
    }
   ],
   "source": [
    "import __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If only one GPU or only CPUs (not recommended but possible) are available, run training with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Process/rank 0 (PID 2568377) acquired port 55441\n",
      "Arguments:\n",
      "\tconfig: _config_files/test_single_gpu.jsonc\n",
      "\thyperparameter_search: False\n",
      "\tdataset_HF: c-bone/mpdb-prop_test_clean\n",
      "\tpretrained_tokenizer_dir: HF-cif-tokenizer\n",
      "\tcontext_length: 1024\n",
      "\tdataset_streaming: False\n",
      "\tremove_CIFs_above_context: False\n",
      "\tremove_CIFs_with_unk: False\n",
      "\tcondition_columns: ['norm_Bandgap']\n",
      "\tn_prefix_tokens: 1\n",
      "\tn_hidden_cond: 1024\n",
      "\tcond_dropout: 0.01\n",
      "\tshare_layers: False\n",
      "\tn_heads_sharing_slider: 2\n",
      "\tcond_lr: 0.0005\n",
      "\tcond_wd: 0.01\n",
      "\tactivate_conditionality: PKV\n",
      "\tn_embd: 512\n",
      "\tn_layer: 8\n",
      "\tn_head: 8\n",
      "\tresidual_dropout: 0.1\n",
      "\tembedding_dropout: 0.1\n",
      "\tattention_dropout: 0.1\n",
      "\ttrain_batch_size: 16\n",
      "\teval_batch_size: 16\n",
      "\tgradient_accumulation_steps: 2\n",
      "\tauto_find_batch_size: False\n",
      "\tlearning_rate: 5e-06\n",
      "\tlr_scheduler_type: cosine_with_min_lr\n",
      "\tlr_scheduler_kwargs: {'min_lr_rate': 0.01}\n",
      "\twarmup_steps: None\n",
      "\twarmup_ratio: 0.1\n",
      "\tadam_beta1: 0.9\n",
      "\tadam_beta2: 0.999\n",
      "\tgrad_clip: 1.0\n",
      "\tweight_decay: 0.01\n",
      "\toutput_dir: model_ckpts/test\n",
      "\tlogging_steps: 50\n",
      "\tsave_total_limit: 2\n",
      "\treport_to: wandb\n",
      "\twandb_project_folder: CrystaLLMv2\n",
      "\tpretrained_model_dir: model_ckpts/mpdb-small-base-lematerial/checkpoint-1250000\n",
      "\teval_strategy: steps\n",
      "\tsave_strategy: steps\n",
      "\teval_steps: 500\n",
      "\tmax_steps: 5000\n",
      "\tearly_stopping_patience: 15\n",
      "\tearly_stopping_threshold: 5e-06\n",
      "\tseed: 1\n",
      "\tdata_seed: 1\n",
      "\tload_best_model_at_end: True\n",
      "\tmetric_for_best_model: eval_loss\n",
      "\tgreater_is_better: False\n",
      "\ttorch_compile: True\n",
      "\tfp16: True\n",
      "\tdeepspeed_config: None\n",
      "\tmodel_ckpt_dir: model_ckpts/cif-gpt2-small/checkpoint-400\n",
      "\tprompt: data_Ca2Fe2As2F2\n",
      "\tdo_sample: True\n",
      "\ttop_k: 50\n",
      "\ttop_p: 0.95\n",
      "\ttemperature: 1.0\n",
      "\tgen_max_length: 1024\n",
      "\tnum_return_sequences: 1\n",
      "\tinput_parquet: None\n",
      "\toutput_parquet: None\n",
      "\tmax_samples: None\n",
      "\tcondition_vector: None\n",
      "\tnum_repeats: 1\n",
      "\tcodecarbon: True\n",
      "\ttracker_project: CrystaLLMv2\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muccacbo\u001b[0m (\u001b[33muccacbo-university-college-london-ucl-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/cyprien/.netrc\n",
      "[codecarbon INFO @ 14:03:13] offline tracker init\n",
      "[codecarbon WARNING @ 14:03:13] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon ERROR @ 14:03:14] You first need to start the tracker.\n",
      "CodeCarbon tracker started\n",
      "You have 1 GPUs\n",
      "No deepspeed config provided. Setting to use only 1 GPU\n",
      "Cache directory: model_ckpts/test/../.cache\n",
      "Tokenizing dataset\n",
      "Tokenizer validation passed: token vocabulary is consistent.\n",
      "\n",
      "**CONDITIONALITY ACTIVATED**\n",
      "Condition type: PKV\n",
      "\n",
      "Building PKV model with n_positions=1025, prefix_tokens=1\n",
      "Loading model weights from model_ckpts/mpdb-small-base-lematerial/checkpoint-1250000\n",
      "Some weights of PKVGPT were not initialized from the model checkpoint at model_ckpts/mpdb-small-base-lematerial/checkpoint-1250000 and are newly initialized: ['conditioning.processor.0.bias', 'conditioning.processor.0.weight', 'conditioning.processor.1.bias', 'conditioning.processor.1.weight', 'conditioning.processor.3.bias', 'conditioning.processor.3.weight', 'conditioning.to_kv.bias', 'conditioning.to_kv.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Loading as PKV with n_positions=1025\n",
      "\n",
      "Resizing positional embeddings with sinusoidal function for position awareness\n",
      "Resized positional embeddings from 1024 to 1025\n",
      "Loaded model from pretrained model directory successfully\n",
      "Base params: 100, Conditioning params: 8\n",
      "/home/cyprien/miniconda3/envs/crystallmv2_venv/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Using different learning rates for base and conditioning params: 5e-06 and 0.0005\n",
      "/home/cyprien/CrystaLLMv2_PKV/_train.py:227: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CIFFormattingTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CIFFormattingTrainer(\n",
      "[2025-09-16 14:03:18,045] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to \u001b[34m\u001b[4mhttps://wandb.me/wandb-init\u001b[0m.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/cyprien/CrystaLLMv2_PKV/wandb/run-20250916_140319-nj8aewcn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmodel_ckpts/test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/uccacbo-university-college-london-ucl-/CrystaLLMv2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/uccacbo-university-college-london-ucl-/CrystaLLMv2/runs/nj8aewcn\u001b[0m\n",
      "  0%|                                                  | 0/5000 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:00<00:00, 16.80it/s]\u001b[A\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 5/10 [00:00<00:00,  9.35it/s]\u001b[A\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 7/10 [00:00<00:00,  7.77it/s]\u001b[A\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 8/10 [00:00<00:00,  7.44it/s]\u001b[A\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 9/10 [00:01<00:00,  7.18it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.7693856954574585, 'eval_runtime': 1.8101, 'eval_samples_per_second': 87.287, 'eval_steps_per_second': 5.525, 'epoch': 0}\n",
      "  0%|                                                  | 0/5000 [00:01<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:01<00:00,  6.92it/s]\u001b[A\n",
      "{'lm_loss': 0.8150806427001953, 'format_loss': 0.3855862021446228}              \u001b[A\n",
      "{'lm_loss': 0.9361668825149536, 'format_loss': 0.45440706610679626}             \n",
      "{'loss': 1.6178, 'grad_norm': 14.851007461547852, 'learning_rate': 4.800000000000001e-07, 'epoch': 2.0}\n",
      "{'lm_loss': 0.520114004611969, 'format_loss': 0.05290163308382034}              \n",
      "{'lm_loss': 0.48872876167297363, 'format_loss': 0.05191105231642723}            \n",
      "  1%|‚ñç                                        | 60/5000 [00:53<43:35,  1.89it/s]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cyprien/CrystaLLMv2_PKV/_train.py\", line 261, in <module>\n",
      "    main()\n",
      "  File \"/home/cyprien/CrystaLLMv2_PKV/_train.py\", line 245, in main\n",
      "    trainer.train()\n",
      "  File \"/home/cyprien/miniconda3/envs/crystallmv2_venv/lib/python3.10/site-packages/transformers/trainer.py\", line 2171, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/cyprien/miniconda3/envs/crystallmv2_venv/lib/python3.10/site-packages/transformers/trainer.py\", line 2536, in _inner_training_loop\n",
      "    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cyprien/CrystaLLMv2_PKV/_train.py\", line 261, in <module>\n",
      "    main()\n",
      "  File \"/home/cyprien/CrystaLLMv2_PKV/_train.py\", line 245, in main\n",
      "    trainer.train()\n",
      "  File \"/home/cyprien/miniconda3/envs/crystallmv2_venv/lib/python3.10/site-packages/transformers/trainer.py\", line 2171, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/cyprien/miniconda3/envs/crystallmv2_venv/lib/python3.10/site-packages/transformers/trainer.py\", line 2536, in _inner_training_loop\n",
      "    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n",
      "KeyboardInterrupt\n",
      "Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7ac0237530a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cyprien/miniconda3/envs/crystallmv2_venv/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py\", line 93, in teardown_atexit\n",
      "    conn.teardown(hooks.exit_code)\n",
      "  File \"/home/cyprien/miniconda3/envs/crystallmv2_venv/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py\", line 210, in teardown\n",
      "    self._client.send(\n",
      "  File \"/home/cyprien/miniconda3/envs/crystallmv2_venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 212, in send\n",
      "    self.send_server_request(server_req)\n",
      "  File \"/home/cyprien/miniconda3/envs/crystallmv2_venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 154, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"/home/cyprien/miniconda3/envs/crystallmv2_venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 151, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/home/cyprien/miniconda3/envs/crystallmv2_venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!python _train.py --config '_config_files/test_single_gpu.jsonc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If multiple GPUs are available, we can run parallelised training (make sure you have a default deepspeed config set up for this, available on github)\n",
    "- Specify nb of GPUs with nproc_per_node argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0916 14:04:18.250000 2570039 site-packages/torch/distributed/run.py:793] \n",
      "W0916 14:04:18.250000 2570039 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0916 14:04:18.250000 2570039 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0916 14:04:18.250000 2570039 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "\n",
      "Process/rank 0 (PID 2570104) acquired port 43243\n",
      "Arguments:\n",
      "\tconfig: _config_files/test.jsonc\n",
      "\thyperparameter_search: False\n",
      "\tdataset_HF: c-bone/mpdb-prop_test_clean\n",
      "\tpretrained_tokenizer_dir: HF-cif-tokenizer\n",
      "\tcontext_length: 1024\n",
      "\tdataset_streaming: False\n",
      "\tremove_CIFs_above_context: False\n",
      "\tremove_CIFs_with_unk: False\n",
      "\tcondition_columns: ['norm_Bandgap']\n",
      "\tn_prefix_tokens: 1\n",
      "\tn_hidden_cond: 1024\n",
      "\tcond_dropout: 0.01\n",
      "\tshare_layers: False\n",
      "\tn_heads_sharing_slider: 2\n",
      "\tcond_lr: 0.0005\n",
      "\tcond_wd: 0.01\n",
      "\tactivate_conditionality: PKV\n",
      "\tn_embd: 512\n",
      "\tn_layer: 8\n",
      "\tn_head: 8\n",
      "\tresidual_dropout: 0.1\n",
      "\tembedding_dropout: 0.1\n",
      "\tattention_dropout: 0.1\n",
      "\ttrain_batch_size: 32\n",
      "\teval_batch_size: 32\n",
      "\tgradient_accumulation_steps: 1\n",
      "\tauto_find_batch_size: False\n",
      "\tlearning_rate: 5e-06\n",
      "\tlr_scheduler_type: cosine_with_min_lr\n",
      "\tlr_scheduler_kwargs: {'min_lr_rate': 0.01}\n",
      "\twarmup_steps: None\n",
      "\twarmup_ratio: 0.1\n",
      "\tadam_beta1: 0.9\n",
      "\tadam_beta2: 0.999\n",
      "\tgrad_clip: 1.0\n",
      "\tweight_decay: 0.01\n",
      "\toutput_dir: model_ckpts/test\n",
      "\tlogging_steps: 50\n",
      "\tsave_total_limit: 2\n",
      "\treport_to: wandb\n",
      "\twandb_project_folder: CrystaLLMv2\n",
      "\tpretrained_model_dir: model_ckpts/mpdb-small-base-lematerial/checkpoint-1250000\n",
      "\teval_strategy: steps\n",
      "\tsave_strategy: steps\n",
      "\teval_steps: 500\n",
      "\tmax_steps: 5000\n",
      "\tearly_stopping_patience: 15\n",
      "\tearly_stopping_threshold: 5e-06\n",
      "\tseed: 1\n",
      "\tdata_seed: 1\n",
      "\tload_best_model_at_end: True\n",
      "\tmetric_for_best_model: eval_loss\n",
      "\tgreater_is_better: False\n",
      "\ttorch_compile: True\n",
      "\tfp16: True\n",
      "\tdeepspeed_config: config_files/deepspeed_default.json\n",
      "\tmodel_ckpt_dir: model_ckpts/cif-gpt2-small/checkpoint-400\n",
      "\tprompt: data_Ca2Fe2As2F2\n",
      "\tdo_sample: True\n",
      "\ttop_k: 50\n",
      "\ttop_p: 0.95\n",
      "\ttemperature: 1.0\n",
      "\tgen_max_length: 1024\n",
      "\tnum_return_sequences: 1\n",
      "\tinput_parquet: None\n",
      "\toutput_parquet: None\n",
      "\tmax_samples: None\n",
      "\tcondition_vector: None\n",
      "\tnum_repeats: 1\n",
      "\tcodecarbon: True\n",
      "\ttracker_project: CrystaLLMv2\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muccacbo\u001b[0m (\u001b[33muccacbo-university-college-london-ucl-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/cyprien/.netrc\n",
      "\n",
      "Process/rank 1 (PID 2570105) acquired port 60205\n",
      "Arguments:\n",
      "\tconfig: _config_files/test.jsonc\n",
      "\thyperparameter_search: False\n",
      "\tdataset_HF: c-bone/mpdb-prop_test_clean\n",
      "\tpretrained_tokenizer_dir: HF-cif-tokenizer\n",
      "\tcontext_length: 1024\n",
      "\tdataset_streaming: False\n",
      "\tremove_CIFs_above_context: False\n",
      "\tremove_CIFs_with_unk: False\n",
      "\tcondition_columns: ['norm_Bandgap']\n",
      "\tn_prefix_tokens: 1\n",
      "\tn_hidden_cond: 1024\n",
      "\tcond_dropout: 0.01\n",
      "\tshare_layers: False\n",
      "\tn_heads_sharing_slider: 2\n",
      "\tcond_lr: 0.0005\n",
      "\tcond_wd: 0.01\n",
      "\tactivate_conditionality: PKV\n",
      "\tn_embd: 512\n",
      "\tn_layer: 8\n",
      "\tn_head: 8\n",
      "\tresidual_dropout: 0.1\n",
      "\tembedding_dropout: 0.1\n",
      "\tattention_dropout: 0.1\n",
      "\ttrain_batch_size: 32\n",
      "\teval_batch_size: 32\n",
      "\tgradient_accumulation_steps: 1\n",
      "\tauto_find_batch_size: False\n",
      "\tlearning_rate: 5e-06\n",
      "\tlr_scheduler_type: cosine_with_min_lr\n",
      "\tlr_scheduler_kwargs: {'min_lr_rate': 0.01}\n",
      "\twarmup_steps: None\n",
      "\twarmup_ratio: 0.1\n",
      "\tadam_beta1: 0.9\n",
      "\tadam_beta2: 0.999\n",
      "\tgrad_clip: 1.0\n",
      "\tweight_decay: 0.01\n",
      "\toutput_dir: model_ckpts/test\n",
      "\tlogging_steps: 50\n",
      "\tsave_total_limit: 2\n",
      "\treport_to: wandb\n",
      "\twandb_project_folder: CrystaLLMv2\n",
      "\tpretrained_model_dir: model_ckpts/mpdb-small-base-lematerial/checkpoint-1250000\n",
      "\teval_strategy: steps\n",
      "\tsave_strategy: steps\n",
      "\teval_steps: 500\n",
      "\tmax_steps: 5000\n",
      "\tearly_stopping_patience: 15\n",
      "\tearly_stopping_threshold: 5e-06\n",
      "\tseed: 1\n",
      "\tdata_seed: 1\n",
      "\tload_best_model_at_end: True\n",
      "\tmetric_for_best_model: eval_loss\n",
      "\tgreater_is_better: False\n",
      "\ttorch_compile: True\n",
      "\tfp16: True\n",
      "\tdeepspeed_config: config_files/deepspeed_default.json\n",
      "\tmodel_ckpt_dir: model_ckpts/cif-gpt2-small/checkpoint-400\n",
      "\tprompt: data_Ca2Fe2As2F2\n",
      "\tdo_sample: True\n",
      "\ttop_k: 50\n",
      "\ttop_p: 0.95\n",
      "\ttemperature: 1.0\n",
      "\tgen_max_length: 1024\n",
      "\tnum_return_sequences: 1\n",
      "\tinput_parquet: None\n",
      "\toutput_parquet: None\n",
      "\tmax_samples: None\n",
      "\tcondition_vector: None\n",
      "\tnum_repeats: 1\n",
      "\tcodecarbon: True\n",
      "\ttracker_project: CrystaLLMv2\n",
      "\n",
      "[codecarbon INFO @ 14:04:23] offline tracker init\n",
      "[codecarbon WARNING @ 14:04:23] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muccacbo\u001b[0m (\u001b[33muccacbo-university-college-london-ucl-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/cyprien/.netrc\n",
      "[codecarbon INFO @ 14:04:24] offline tracker init\n",
      "[codecarbon WARNING @ 14:04:24] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon ERROR @ 14:04:24] You first need to start the tracker.\n",
      "CodeCarbon tracker started\n",
      "Cache directory: model_ckpts/test/../.cache\n",
      "[codecarbon ERROR @ 14:04:25] You first need to start the tracker.\n",
      "CodeCarbon tracker started\n",
      "Cache directory: model_ckpts/test/../.cache\n",
      "Tokenizing dataset\n",
      "Tokenizer validation passed: token vocabulary is consistent.\n",
      "\n",
      "**CONDITIONALITY ACTIVATED**\n",
      "Condition type: PKV\n",
      "Tokenizing dataset\n",
      "Tokenizer validation passed: token vocabulary is consistent.\n",
      "\n",
      "**CONDITIONALITY ACTIVATED**\n",
      "Condition type: PKV\n",
      "\n",
      "Building PKV model with n_positions=1025, prefix_tokens=1\n",
      "Loading model weights from model_ckpts/mpdb-small-base-lematerial/checkpoint-1250000\n",
      "\n",
      "Building PKV model with n_positions=1025, prefix_tokens=1\n",
      "Loading model weights from model_ckpts/mpdb-small-base-lematerial/checkpoint-1250000\n",
      "Some weights of PKVGPT were not initialized from the model checkpoint at model_ckpts/mpdb-small-base-lematerial/checkpoint-1250000 and are newly initialized: ['conditioning.processor.0.bias', 'conditioning.processor.0.weight', 'conditioning.processor.1.bias', 'conditioning.processor.1.weight', 'conditioning.processor.3.bias', 'conditioning.processor.3.weight', 'conditioning.to_kv.bias', 'conditioning.to_kv.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Loading as PKV with n_positions=1025\n",
      "Some weights of PKVGPT were not initialized from the model checkpoint at model_ckpts/mpdb-small-base-lematerial/checkpoint-1250000 and are newly initialized: ['conditioning.processor.0.bias', 'conditioning.processor.0.weight', 'conditioning.processor.1.bias', 'conditioning.processor.1.weight', 'conditioning.processor.3.bias', 'conditioning.processor.3.weight', 'conditioning.to_kv.bias', 'conditioning.to_kv.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Resizing positional embeddings with sinusoidal function for position awareness\n",
      "\n",
      "Loading as PKV with n_positions=1025\n",
      "\n",
      "Resizing positional embeddings with sinusoidal function for position awareness\n",
      "Resized positional embeddings from 1024 to 1025\n",
      "Loaded model from pretrained model directory successfully\n",
      "Resized positional embeddings from 1024 to 1025\n",
      "Loaded model from pretrained model directory successfully\n",
      "Base params: 100, Conditioning params: 8\n",
      "/home/cyprien/miniconda3/envs/crystallmv2_venv/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Using different learning rates for base and conditioning params: 5e-06 and 0.0005\n",
      "/home/cyprien/CrystaLLMv2_PKV/_train.py:227: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CIFFormattingTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CIFFormattingTrainer(\n",
      "Base params: 100, Conditioning params: 8\n",
      "/home/cyprien/miniconda3/envs/crystallmv2_venv/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Using different learning rates for base and conditioning params: 5e-06 and 0.0005\n",
      "/home/cyprien/CrystaLLMv2_PKV/_train.py:227: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CIFFormattingTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CIFFormattingTrainer(\n",
      "[2025-09-16 14:04:28,498] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-16 14:04:28,540] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to \u001b[34m\u001b[4mhttps://wandb.me/wandb-init\u001b[0m.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/cyprien/CrystaLLMv2_PKV/wandb/run-20250916_140430-rvjrlnso\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmodel_ckpts/test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/uccacbo-university-college-london-ucl-/CrystaLLMv2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/uccacbo-university-college-london-ucl-/CrystaLLMv2/runs/rvjrlnso\u001b[0m\n",
      "  0%|                                                  | 0/5000 [00:00<?, ?it/s]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.8096067905426025, 'eval_runtime': 1.7586, 'eval_samples_per_second': 89.844, 'eval_steps_per_second': 1.706, 'epoch': 0}\n",
      "  0%|                                                  | 0/5000 [00:01<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  6.99it/s]\u001b[A\n",
      "                                                                                \u001b[A[rank0]:W0916 14:04:33.054000 2570104 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n",
      "[rank1]:W0916 14:04:33.059000 2570105 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n",
      "[rank0]:[W916 14:04:38.460837770 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank1]:[W916 14:04:38.532909836 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "{'lm_loss': 0.9003651738166809, 'format_loss': 0.3965293765068054}              \n",
      "{'loss': 0.8387, 'grad_norm': 4.288746356964111, 'learning_rate': 4.7000000000000005e-07, 'epoch': 3.85}\n",
      "{'lm_loss': 0.4379175305366516, 'format_loss': 0.04060088470578194}             \n",
      "  2%|‚ñã                                        | 79/5000 [00:55<50:11,  1.63it/s]W0916 14:05:27.346000 2570039 site-packages/torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGINT death signal, shutting down workers\n",
      "W0916 14:05:27.347000 2570039 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2570104 closing signal SIGINT\n",
      "W0916 14:05:27.347000 2570039 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2570105 closing signal SIGINT\n",
      "^C\n",
      "W0916 14:05:27.398000 2570039 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2570104 closing signal SIGTERM\n",
      "W0916 14:05:27.398000 2570039 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2570105 closing signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "# if we want to run on 2 GPUs\n",
    "!torchrun --nproc_per_node=2 _train.py --config '_config_files/test.jsonc'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystallmv2_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


import torch
from transformers.models.gpt2.modeling_gpt2 import (
    GPT2Attention,
    GPT2Block as HFGPT2Block, # renamed to avoid confusion with our own Block
    GPT2MLP,
    load_tf_weights_in_gpt2,
    Conv1D,
    GPT2_INPUTS_DOCSTRING,
    _prepare_4d_causal_attention_mask_for_sdpa,
    _prepare_4d_attention_mask_for_sdpa,
    add_start_docstrings_to_model_forward,
    add_code_sample_docstrings,
    add_start_docstrings,
    _CHECKPOINT_FOR_DOC,
    _CONFIG_FOR_DOC,
    PARALLELIZE_DOCSTRING,
    DEPARALLELIZE_DOCSTRING,
    assert_device_map,
    get_device_map,
    GPT2_START_DOCSTRING,
    ALL_ATTENTION_FUNCTIONS,
    eager_attention_forward,
)
from transformers import GPT2Config, GPT2PreTrainedModel, GenerationMixin
from torch.nn import CrossEntropyLoss
import math
from typing import Optional, Tuple, Union, Callable
import warnings
import numpy as np
from transformers.utils import logging
import torch.nn as nn
from transformers.modeling_outputs import CausalLMOutputWithPast, BaseModelOutputWithPast


logger = logging.get_logger(__name__)

# Define the marker for missing condition values
MISSING_CONDITION_VALUE = -100.0

class SliderGPT2Config(GPT2Config):
    """
    Inherit from GPT2Config to add slider-specific parameters.
    Args:
        slider_on (bool): Whether to use the slider model.
        slider_n_variables (int): this nb will be extracted fromm the condition vector directly
                                how many comma seperated variables are in the condition vector.
        slider_n_hidden (int): Hidden layer size in the prefix encoder (MLP).
        slider_n_heads_sharing_slider (int):  Crucial for parameter efficiency.
                                It dictates how many standard attention heads within a layer
                                will share the same key-value pairs generated by the SliderEncoder.
                                For example, if num_attention_heads is 12 and slider_n_heads_sharing_slider
                                is 3, the SliderEncoder only needs to generate K/V for 12/3 = 4
                                "slider heads", which are then replicated across the 12 base heads.
                                If set to 1, each base head gets a unique slider K/V projection.
                                Must divide num_attention_heads evenly.
        slider_dropout (float): Dropout rate in the prefix encoder.
    """
    # model_type = "slider_gpt2"

    def __init__(self,
                 slider_on=False,
                 slider_n_variables=1,
                 slider_n_hidden=768,
                 slider_n_heads_sharing_slider=1,
                 slider_dropout=0.1,
                 **kwargs):
        super().__init__(**kwargs)
        self.slider_on = slider_on
        self.slider_n_variables = slider_n_variables
        self.slider_n_hidden = slider_n_hidden
        self.slider_n_heads_sharing_slider = slider_n_heads_sharing_slider
        self.slider_dropout = slider_dropout

class SliderEncoder(nn.Module):
    def __init__(self, config: SliderGPT2Config):
        """
        A model that encodes slider variables into attention key-value pairs.

        Args:
            config (SliderGPT2Config): Configuration object containing slider parameters.
        """
        super().__init__()

        self.n_variables = config.slider_n_variables
        self.n_hidden = config.slider_n_hidden
        self.n_heads_sharing_slider = config.slider_n_heads_sharing_slider
        self.dropout_rate = config.slider_dropout
        self.n_base_heads = config.num_attention_heads
        if config.hidden_size % self.n_base_heads != 0:
             raise ValueError(
                 f"hidden_size ({config.hidden_size}) must be divisible by "
                 f"num_attention_heads ({self.n_base_heads})"
             )
        self.n_token_dim = config.hidden_size // self.n_base_heads # This is head_dim

        self.register_buffer('dummy', torch.empty(0))

        if self.n_base_heads % self.n_heads_sharing_slider != 0:
            raise ValueError(
                f"n_base_heads ({self.n_base_heads}) must be divisible by "
                f"n_heads_sharing_slider ({self.n_heads_sharing_slider})."
            )

        self.n_slider_heads = self.n_base_heads // self.n_heads_sharing_slider
        self.kv_size = 2 * self.n_token_dim * self.n_slider_heads

        self.encode_linear = nn.Linear(1, self.n_variables * self.kv_size)
        self.upscale_linear = nn.Linear(self.kv_size, self.n_variables * self.n_hidden)
        self.downscale_linear = nn.Linear(self.n_hidden, self.n_variables * self.kv_size)

        self.attention_factor = nn.Linear(1, 1, bias=False)
        self.attention_factor.SLIDER_DO_NOT_REINITIALIZE = True

        self.tanh = nn.Tanh()
        self.dropout = nn.Dropout(self.dropout_rate)

        # print("Doing 0-initialization of SliderEncoder projection weights FOR TESTING.")
        # logger.info("DEBUG: Zero-initializing SliderEncoder projection weights FOR TESTING.")
        # nn.init.zeros_(self.encode_linear.weight)
        # if self.encode_linear.bias is not None:
        #     nn.init.zeros_(self.encode_linear.bias)
        
        # nn.init.zeros_(self.upscale_linear.weight)
        # if self.upscale_linear.bias is not None:
        #     nn.init.zeros_(self.upscale_linear.bias)

        # nn.init.zeros_(self.downscale_linear.weight)
        # if self.downscale_linear.bias is not None:
        #     nn.init.zeros_(self.downscale_linear.bias)

    def forward(self, prefix: torch.Tensor, hidden_states: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass for generating key-value pairs from slider variables.

        Args:
            prefix (Tensor): Input slider values of shape [batch_size, n_variables].
                             May contain MISSING_CONDITION_VALUE.
            hidden_states (Optional[Tensor]): Used to determine device and dtype if provided.

        Returns:
            Tuple[Tensor, Tensor, Tensor, Tensor]:
                - slider_keys: Shape [batch_size, n_base_heads, n_variables, n_token_dim].
                - slider_values: Shape [batch_size, n_base_heads, n_variables, n_token_dim].
                - slider_factor: Scalar tensor.
                - condition_mask: Boolean tensor of shape [batch_size, n_variables],
                                  True where condition was present.
        """
        device = hidden_states.device if hidden_states is not None else self.dummy.device
        dtype = hidden_states.dtype if hidden_states is not None else self.dummy.dtype
        prefix = prefix.to(device=device, dtype=dtype)

        # Create mask for present conditions *before* replacing the marker (do +- 0.1 to avoid numerical issues)
        condition_mask = (prefix != MISSING_CONDITION_VALUE) & (prefix > -100.1) & (prefix < 100.1)
        # print(f"condition_mask: {condition_mask}")

        # Replace marker with 0.0 for MLP processing (bias will still activate)
        # Alternatively, could leave marker, but 0.0 is numerically safer.
        prefix_processed = torch.where(condition_mask, prefix, torch.zeros_like(prefix))

        prefix_processed = prefix_processed.unsqueeze(-1) # Shape: [batch_size, n_variables, 1]

        encode_w = self.encode_linear.weight.view(self.n_variables, self.kv_size, 1)
        encode_b = self.encode_linear.bias.view(1, self.n_variables, self.kv_size)
        slider_kv = torch.einsum("VKI,BVI->BVK", encode_w, prefix_processed) + encode_b
        slider_kv = self.tanh(slider_kv)
        slider_kv = self.dropout(slider_kv)

        upscale_w = self.upscale_linear.weight.view(self.n_variables, self.n_hidden, self.kv_size)
        upscale_b = self.upscale_linear.bias.view(1, self.n_variables, self.n_hidden)
        slider_kv = torch.einsum("VHK,BVK->BVH", upscale_w, slider_kv) + upscale_b
        slider_kv = self.tanh(slider_kv)
        slider_kv = self.dropout(slider_kv)

        downscale_w = self.downscale_linear.weight.view(self.n_variables, self.kv_size, self.n_hidden)
        downscale_b = self.downscale_linear.bias.view(1, self.n_variables, self.kv_size)
        slider_kv = torch.einsum("VKH,BVH->BVK", downscale_w, slider_kv) + downscale_b

        # Shape: [batch_size, n_variables, 2, n_slider_heads, n_token_dim]
        slider_kv = slider_kv.view(prefix.shape[0], self.n_variables, 2, self.n_slider_heads, self.n_token_dim)

        # Shape: [batch_size, n_variables, 2, n_base_heads, n_token_dim]
        slider_kv = slider_kv.repeat_interleave(self.n_heads_sharing_slider, dim=3)
        assert slider_kv.shape[3] == self.n_base_heads, (
            f"Head expansion failed: expected {self.n_base_heads} heads, "
            f"got {slider_kv.shape[3]} (n_base_heads={self.n_base_heads}, "
            f"n_heads_sharing_slider={self.n_heads_sharing_slider})"
        )

        # Permute to [batch_size, n_base_heads, n_variables, n_token_dim, 2]
        slider_kv = slider_kv.permute(0, 3, 1, 4, 2)

        slider_keys, slider_values = slider_kv[..., 0], slider_kv[..., 1]

        # Tensor shapes
        # prefix goes from [batch_size, n_variables, 1] to [batch_size, n_variables, kv_size]
        # kvs passed through MLP to [batch_size, n_variables, n_hidden]
        # kvs go back down to [batch_size, n_variables, kv_size]
        # then reshaped to [batch_size, n_variables, 2, n_slider_heads, n_token_dim]
        # then repeated to [batch_size, n_variables, 2, n_base_heads, n_token_dim]
        # and finally permuted to [batch_size, n_base_heads, seq_len, n_token_dim, 2]

        # ######################################################################
        # Debugging: Check for NaN or Inf values in the tensors
        # Inside SliderEncoder.forward, before return
        # if torch.distributed.get_rank() == 0: # Print once per step from rank 0
        #     print(f"DEBUG SliderEncoder Input prefix sum: {prefix.sum().item()}, isnan: {torch.isnan(prefix).any()}, isinf: {torch.isinf(prefix).any()}, dtype: {prefix.dtype}", flush=True)
        #     print(f"DEBUG SliderEncoder prefix_processed sum: {prefix_processed.sum().item()}", flush=True)
        #     print(f"DEBUG SliderEncoder K sum: {slider_keys.sum().item()}, K isnan: {torch.isnan(slider_keys).any()}, K isinf: {torch.isinf(slider_keys).any()}", flush=True)
        #     print(f"DEBUG SliderEncoder V sum: {slider_values.sum().item()}, V isnan: {torch.isnan(slider_values).any()}, V isinf: {torch.isinf(slider_values).any()}", flush=True)
        #     print(f"DEBUG SliderEncoder Factor: {self.attention_factor.weight[0, 0].item()}", flush=True)

        return slider_keys, slider_values, self.attention_factor.weight[0, 0], condition_mask

class SliderAttention(GPT2Attention):
    """
    Replaces the standard GPT2Attention. It performs the standard self-attention calculation
    then, if sliders are enabled, computes and adds the slider-based attention contribution,
    masking out contributions from missing condition variables.
    """

    def __init__(self, config, is_cross_attention=False, layer_idx=None):
        super().__init__(config, is_cross_attention=is_cross_attention, layer_idx=layer_idx)
        self.config = config
        # Store layer_idx for print statements
        self.layer_idx = layer_idx if layer_idx is not None else "Unknown"

    def forward(
        self,
        hidden_states: torch.Tensor,
        layer_past: Optional[Tuple[torch.Tensor]] = None,
        attention_mask: Optional[torch.Tensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        use_cache: bool = False,
        output_attentions: bool = False,
        slider_key_value_factor_mask: Optional[Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]] = None,
        **kwargs,
    ) -> Tuple[torch.Tensor, ...]:
        """
        Forward pass including slider attention logic with masking and visualization.
        """
        # =========================
        # Original GPT2Attention logic
        # =========================
        # Determine rank for distributed training print clarity
        rank = torch.distributed.get_rank() if torch.distributed.is_initialized() else 0

        query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2)

        shape_q = (*query_states.shape[:-1], -1, self.head_dim)
        shape_kv = (*key_states.shape[:-1], -1, self.head_dim)

        query_states = query_states.view(shape_q).transpose(1, 2) # [B, H, N, Z]
        key_states = key_states.view(shape_kv).transpose(1, 2)
        value_states = value_states.view(shape_kv).transpose(1, 2)

        if layer_past is not None:
            past_key, past_value = layer_past
            key_states = torch.cat((past_key, key_states), dim=-2)
            value_states = torch.cat((past_value, value_states), dim=-2)

        present = (key_states, value_states) if use_cache else None

        is_causal = attention_mask is None and query_states.shape[-2] > 1

        using_eager = self.config._attn_implementation == "eager"
        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            if self.config._attn_implementation == "sdpa" and (output_attentions or head_mask is not None):
                using_eager = True
                logger.warning_once(
                    "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to "
                    'eager attention. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.'
                )
            else:
                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        if using_eager and self.reorder_and_upcast_attn:
            attn_output, attn_weights = self._upcast_and_reordered_attn(
                query_states, key_states, value_states, attention_mask, head_mask
            )
        else:
            attn_output, attn_weights = attention_interface(
                self,
                query_states,
                key_states,
                value_states,
                attention_mask,
                head_mask=head_mask,
                dropout=self.attn_dropout.p if self.training else 0.0,
                is_causal=is_causal,
            )

        attn_output = attn_output.reshape(*attn_output.shape[:-2], -1).contiguous()
        attn_output = self.c_proj(attn_output)
        attn_output = self.resid_dropout(attn_output)

        # ========================
        # Slider logic with Masking & Visualization
        # ========================
        slider_attn_output_2d = 0.0 # Default to zero contribution if slider is off or all masked

        if getattr(self.config, "slider_on", False):
            if slider_key_value_factor_mask is None:
                # This path might be hit if called without conditions during generation,
                # assuming SliderGPT2Model handles the None case by creating default markers.
                # We can add a warning or simply proceed assuming zero contribution is desired.
                logger.warning_once(f"[Rank {rank} Layer {self.layer_idx}] Slider is ON but slider_key_value_factor_mask is None. Skipping slider contribution.")
            else:
                slider_key, slider_value, slider_factor, condition_mask = slider_key_value_factor_mask
                # slider_key: [B, H, M, Z], slider_value: [B, H, M, Z]
                # query_states: [B, H, N, Z]
                # condition_mask: [B, M]

                bsz, num_heads, seq_len_q, head_dim = query_states.shape
                _bsz_s, _h_s, num_vars, _hd_s = slider_key.shape

                # Basic validation (already present, good practice)
                assert bsz == _bsz_s, "Batch size mismatch"
                assert num_heads == _h_s, "Head count mismatch"
                assert head_dim == _hd_s, "Head dim mismatch"
                assert condition_mask.shape == (bsz, num_vars), f"Condition mask shape mismatch. Expected {(bsz, num_vars)}, got {condition_mask.shape}"

                # Calculate raw attention scores: Q * K_slider^T
                # einsum: [B, H, N, Z], [B, H, M, Z] -> [B, H, N, M]
                slider_attn_scores = torch.einsum("BHNZ,BHMZ->BHNM", query_states, slider_key)

                # # Only print from rank 0 and if tensor is not empty to avoid clutter/errors
                # if rank == 0 and slider_attn_scores.numel() > 0:
                #     print(f"--- [Slider Viz Rank {rank} Layer {self.layer_idx}] ---")
                #     print(f"Condition Mask (B={bsz}, M={num_vars}) [0,:]: {condition_mask[0, :]}")
                #     print(f"Raw Scores (B,H,N,M) {slider_attn_scores.shape} [0,0,0,:]: {slider_attn_scores[0, 0, 0, :].detach().cpu().numpy()}") # Slice: 1st batch, 1st head, 1st query token, all conditions M

                # Apply the condition mask
                # Expand mask from [B, M] to [B, 1, 1, M] for broadcasting
                mask_expanded = condition_mask.unsqueeze(1).unsqueeze(2) # Shape: [B, 1, 1, M]
                # Where mask is False (condition was missing), set score to large negative value
                slider_attn_scores = torch.where(
                    mask_expanded,
                    slider_attn_scores,
                    torch.full_like(slider_attn_scores, torch.finfo(slider_attn_scores.dtype).min)
                )

                # if rank == 0 and slider_attn_scores.numel() > 0:
                #      # Use .item() for single value check if possible, or check a slice
                #      all_masked_slice = not mask_expanded[0,0,0,:].any() # Check if all conditions masked for this slice
                #      print(f"Masked Scores [0,0,0,:] (All masked? {all_masked_slice}): {slider_attn_scores[0, 0, 0, :].detach().cpu().numpy()}")

                # Softmax over the slider variables (dimension M)
                # Divide by sqrt(head_dim) *before* softmax for scaling
                slider_attn_weights = torch.softmax(
                    slider_attn_scores / math.sqrt(head_dim), dim=-1 # Softmax over M
                )

                # for potential NaN from softmax(-inf)
                # If all scores were -inf, softmax might yield NaN. Convert NaNs to 0.
                slider_attn_weights = torch.nan_to_num(slider_attn_weights, nan=0.0)

                # if rank == 0 and slider_attn_weights.numel() > 0:
                #      print(f"Softmax Weights [0,0,0,:] (Sum={slider_attn_weights[0,0,0,:].sum():.4f}): {slider_attn_weights[0, 0, 0, :].detach().cpu().numpy()}")
                #      print(f"--- [End Slider Viz Rank {rank} Layer {self.layer_idx}] ---")

                # Multiply weights by slider values V_slider
                # einsum: [B, H, N, M], [B, H, M, Z] -> [B, H, N, Z]
                slider_attn_output = torch.einsum("BHNM,BHMZ->BHNZ", slider_attn_weights, slider_value)

                # Transpose and reshape to match original attn_output format [B, N, H*Z]
                slider_attn_output = slider_attn_output.transpose(1, 2).contiguous() # [B, N, H, Z]
                slider_attn_output_2d = slider_attn_output.reshape(bsz, seq_len_q, num_heads * head_dim) # [B, N, EmbedDim]


                # Check if ALL conditions were masked for each item in the batch
                # condition_mask shape: [B, M]
                # .all(dim=1) checks if all values in the M dimension are True.
                # We want to know where all are False, so we check `~condition_mask.any(dim=1)`
                all_conditions_masked_per_batch = ~condition_mask.any(dim=1) # Shape: [B] (True if all masked for that batch item)

                # If there are any batch items where all conditions were masked...
                if all_conditions_masked_per_batch.any():
                    # Create a mask for broadcasting to slider_attn_output_2d's shape [B, N, EmbedDim]
                    # We want to select rows (batch items) where all_conditions_masked_per_batch is True
                    # Need shape [B, 1, 1] to broadcast correctly with [B, N, EmbedDim]
                    zeroing_mask_b = all_conditions_masked_per_batch.view(bsz, 1, 1)

                    # Where the mask is True, set the output to zero. Otherwise, keep original value.
                    # Note: Using torch.where requires tensors to be on the same device.
                    slider_attn_output_2d = torch.where(
                        zeroing_mask_b, # Condition: True where all conditions were masked
                        torch.zeros_like(slider_attn_output_2d), # Value if True: Zero tensor
                        slider_attn_output_2d # Value if False: Original calculated value
                    )


                # Apply the learned scaling factor ONLY IF slider output is potentially non-zero
                # (This check is still useful for cases where some conditions are present but result is small)
                # Use a slightly larger tolerance to be safe with floats
                if slider_attn_output_2d.abs().sum() > 1e-6:
                    slider_attn_output_2d = slider_attn_output_2d * slider_factor
                else:
                    # Ensure it's exactly zero if sum is below tolerance
                    slider_attn_output_2d = torch.zeros_like(slider_attn_output_2d)

            # print(f"Slider attention output shape: {slider_attn_output_2d.shape}")
            #print sum of slider_attn_output_2d
            # print(f"Slider attention output sum: {slider_attn_output_2d.sum()}")
            # Add slider attention contribution (will now be reliably 0.0 if all conditions were masked)
            
            ####################################################
            # if torch.distributed.get_rank() == 0:
            #     if slider_key_value_factor_mask is not None:
            #         _sk, _sv, _sf, _sm = slider_key_value_factor_mask
            #         print(f"DEBUG SliderAttention Input K sum: {_sk.sum().item()}, isnan: {torch.isnan(_sk).any()}, isinf: {torch.isinf(_sk).any()}", flush=True)
            #         print(f"DEBUG SliderAttention Input V sum: {_sv.sum().item()}, isnan: {torch.isnan(_sv).any()}, isinf: {torch.isinf(_sv).any()}", flush=True)
            #         print(f"DEBUG SliderAttention Input Factor: {_sf.item()}", flush=True)
                # print(f"DEBUG SliderAttention slider_attn_output_2d sum: {slider_attn_output_2d.sum().item()}, isnan: {torch.isnan(slider_attn_output_2d).any()}, isinf: {torch.isinf(slider_attn_output_2d).any()}", flush=True)
                        
            
            attn_output = attn_output + slider_attn_output_2d

            # print the slider_attn_output_2d to see if it does in fact start at 0
            # print(f"Slider attention output 2D shape: {slider_attn_output_2d.shape}")
            # print(f"Slider attention output 2D sum: {slider_attn_output_2d.sum()}")
            # print(f"Slider attention output 2D: {slider_attn_output_2d}")

        outputs = (attn_output, present)
        if output_attentions:
            # Note: Returning original self-attention weights. Slider weights are internal.
            outputs += (attn_weights,)

        return outputs
    

class SliderGPT2Block(HFGPT2Block): # Inherit from renamed HF Block
    """
    A subclass of GPT2Block that adds the 'slider' logic to pass key/value/factor/mask
    into self.attn when `config.slider_on` is True.
    """

    def __init__(self, config, layer_idx=None):
        """
        Initialize the block with the given configuration and layer index.
        """
        super(HFGPT2Block, self).__init__() # Call grandparent's init explicitly

        hidden_size = config.hidden_size
        inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size

        self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
        self.mlp = GPT2MLP(inner_dim, config)
        self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)

        # Use custom SliderAttention
        self.attn = SliderAttention(config, layer_idx=layer_idx)

        # Initialize SliderEncoder conditionally
        if getattr(config, "slider_on", False):
            self.slider = SliderEncoder(config)
            # Add specific check for n_variables during init
            if not hasattr(config, 'slider_n_variables') or config.slider_n_variables <= 0:
                 raise ValueError("config.slider_n_variables must be set and positive if slider_on=True")
        else:
            self.slider = None


    def forward(
        self,
        hidden_states: Optional[Tuple[torch.FloatTensor]],
        layer_past: Optional[Tuple[torch.Tensor]] = None,
        attention_mask: Optional[torch.FloatTensor] = None,
        head_mask: Optional[torch.FloatTensor] = None,
        encoder_hidden_states: Optional[torch.Tensor] = None,
        encoder_attention_mask: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = False,
        output_attentions: Optional[bool] = False,
         # Extra argument for slider variables:
        condition_values: Optional[torch.Tensor] = None,
    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:
        """
        Forward pass. Optionally computes slider key/value/factor/mask and passes it to self.attn.
        """
        if encoder_hidden_states is not None:
             # TODO: Cross attention logic for sliders if needed? Currently assumes self-attention only.
             raise NotImplementedError("Cross-attention with sliders is not implemented.")

        residual = hidden_states
        hidden_states = self.ln_1(hidden_states)

        # =========================
        # Slider logic
        # =========================
        slider_key_value_factor_mask = None
        if self.slider is not None:
            if condition_values is None:
                # This case should be handled by SliderGPT2Model creating a default tensor
                # If it still happens, something is wrong upstream.
                raise ValueError("condition_values is None inside SliderGPT2Block despite slider being active.")
            # SliderEncoder now returns 4 items
            slider_key_value_factor_mask = self.slider(condition_values, hidden_states=hidden_states)

        # ========================
        # Attention call with slider info passed in
        # ========================
        attn_outputs = self.attn(
            hidden_states,
            layer_past=layer_past,
            attention_mask=attention_mask,
            head_mask=head_mask,
            use_cache=use_cache,
            output_attentions=output_attentions,
            # Pass the tuple containing slider_key, value, factor, mask
            slider_key_value_factor_mask=slider_key_value_factor_mask,
        )
        attn_output = attn_outputs[0] # output_attn: a, present, (attentions)
        outputs = attn_outputs[1:]
        # residual connection
        hidden_states = attn_output + residual

        # MLP part
        residual = hidden_states
        hidden_states = self.ln_2(hidden_states)
        feed_forward_hidden_states = self.mlp(hidden_states)
        # residual connection
        hidden_states = residual + feed_forward_hidden_states

        if use_cache:
            outputs = (hidden_states,) + outputs
        else:
            outputs = (hidden_states,) + outputs[1:]

        return outputs # hidden_states, present, (attentions)


class SliderGPT2PreTrainedModel(GPT2PreTrainedModel):
    """
    A subclass of GPT2PreTrainedModel that adds the slider-specific
    initialization logic, mirroring the approach from Qwen2PreTrainedModel.
    """
    config_class = SliderGPT2Config
    load_tf_weights = load_tf_weights_in_gpt2
    base_model_prefix = "transformer"
    is_parallelizable = True
    supports_gradient_checkpointing = True
    # Ensure SliderGPT2Block isn't split during model parallelization
    _no_split_modules = ["SliderGPT2Block"]
    _skip_keys_device_placement = "past_key_values"
    _supports_flash_attn_2 = True # Inherited check
    _supports_sdpa = True # Inherited check

    def __init__(self, *inputs, **kwargs):
        super().__init__(*inputs, **kwargs)

    def _init_weights(self, module):
        """
        Initialize the weights:
        - If `module` has SLIDER_DO_NOT_REINITIALIZE, zero them out.
        - Else, fall back to GPT-2's standard weight initialization logic.
        """
        if hasattr(module, "SLIDER_DO_NOT_REINITIALIZE") and module.SLIDER_DO_NOT_REINITIALIZE:
            module.weight.data.zero_()
            if getattr(module, "bias", None) is not None:
                module.bias.data.zero_()
        elif isinstance(module, (nn.Linear, Conv1D)):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

        for name, p in module.named_parameters():
            # Apply GPT-2 specific scaling for residual projection weights
            if name == "c_proj.weight":
                 # Check if this parameter belongs to an Attention layer (GPT2Attention or SliderAttention)
                 # This check might need refinement if c_proj is used elsewhere with the same name
                 if isinstance(module, (GPT2Attention, SliderAttention)):
                    p.data.normal_(mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.n_layer)))


@add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)
class SliderGPT2Model(SliderGPT2PreTrainedModel):
    """
    The base SliderGPT2 model transformer. Handles input processing and passes
    data through SliderGPT2Blocks. Manages the condition_values tensor,
    creating a default if None is provided when slider_on=True.
    """
    _supports_param_buffer_assignment = False # As per original GPT2Model

    def __init__(self, config: SliderGPT2Config):
        super().__init__(config)

        self.embed_dim = config.hidden_size

        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)
        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)

        self.drop = nn.Dropout(config.embd_pdrop)
        # Use SliderGPT2Block instead of the original GPT2Block
        self.h = nn.ModuleList([SliderGPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])
        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)

        self.model_parallel = False
        self.device_map = None
        self.gradient_checkpointing = False
        # Store attn implementation choice
        self._attn_implementation = config._attn_implementation

        # Initialize weights and apply final processing
        self.post_init()

    @add_start_docstrings(PARALLELIZE_DOCSTRING)
    def parallelize(self, device_map=None):
        warnings.warn(
            "`GPT2Model.parallelize` is deprecated and will be removed in v5 of Transformers, you should load your"
            " model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own"
            " `device_map` but it needs to be a dictionary module_name to device, so for instance {'h.0': 0, 'h.1': 1,"
            " ...}",
            FutureWarning,
        )
        self.device_map = (
            get_device_map(len(self.h), range(torch.cuda.device_count())) if device_map is None else device_map
        )
        assert_device_map(self.device_map, len(self.h))
        self.model_parallel = True
        self.first_device = "cpu" if "cpu" in self.device_map.keys() else "cuda:" + str(min(self.device_map.keys()))
        self.last_device = "cuda:" + str(max(self.device_map.keys()))
        self.wte = self.wte.to(self.first_device)
        self.wpe = self.wpe.to(self.first_device)
        for k, v in self.device_map.items():
            # Deprecated way - needs list of indices per device
            # Correct way needs module name mapping
            # Assuming v is list of layer indices for device k
             for block_idx in v:
                cuda_device = "cuda:" + str(k)
                self.h[block_idx] = self.h[block_idx].to(cuda_device)

        self.ln_f = self.ln_f.to(self.last_device)
        # Input embeddings should be on the first device
        self.wte = self.wte.to(self.first_device)
        self.wpe = self.wpe.to(self.first_device)

    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)
    def deparallelize(self):
        warnings.warn(
            "Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.",
            FutureWarning,
        )
        self.model_parallel = False
        self.device_map = None
        self.first_device = "cpu"
        self.last_device = "cpu"
        self.wte = self.wte.to("cpu")
        self.wpe = self.wpe.to("cpu")
        for index in range(len(self.h)):
            self.h[index] = self.h[index].to("cpu")
        self.ln_f = self.ln_f.to("cpu")
        torch.cuda.empty_cache()


    def get_input_embeddings(self):
        return self.wte

    def set_input_embeddings(self, new_embeddings):
        self.wte = new_embeddings

    def _prune_heads(self, heads_to_prune):
        """
        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer}
        """
        for layer, heads in heads_to_prune.items():
            self.h[layer].attn.prune_heads(heads)


    @add_code_sample_docstrings(
        checkpoint=_CHECKPOINT_FOR_DOC,
        output_type=BaseModelOutputWithPast,
        config_class=_CONFIG_FOR_DOC,
    )
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
        attention_mask: Optional[torch.FloatTensor] = None,
        token_type_ids: Optional[torch.LongTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        head_mask: Optional[torch.FloatTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        encoder_hidden_states: Optional[torch.Tensor] = None, # Added for HF compatibility
        encoder_attention_mask: Optional[torch.FloatTensor] = None, # Added for HF compatibility
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        # Extra argument for slider variables:
        condition_values: Optional[torch.Tensor] = None,
    ) -> Union[Tuple, BaseModelOutputWithPast]:
        """
        Forward pass for the base SliderGPT2Model.
        """
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if input_ids is not None and inputs_embeds is not None:
            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
        elif input_ids is not None:
            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)
            input_shape = input_ids.size()
            batch_size = input_ids.shape[0]
            device = input_ids.device
        elif inputs_embeds is not None:
            input_shape = inputs_embeds.size()[:-1]
            batch_size = inputs_embeds.shape[0]
            device = inputs_embeds.device
        else:
            raise ValueError("You have to specify either input_ids or inputs_embeds")

        # =========================
        # Slider condition handling
        # =========================
        processed_condition_values = None
        if getattr(self.config, "slider_on", False):
            if condition_values is None:
                # If slider is on but no conditions provided, create a tensor of missing markers
                logger.warning_once("slider_on=True but condition_values=None. Using default missing values.")
                processed_condition_values = torch.full(
                    (batch_size, self.config.slider_n_variables),
                    MISSING_CONDITION_VALUE,
                    dtype=torch.float32, # Assuming float conditions
                    device=device
                )
            else:
                # Validate provided condition_values
                if condition_values.dim() != 2 or condition_values.shape[0] != batch_size:
                     raise ValueError(f"condition_values must be 2D tensor of shape (batch_size, n_variables). Got {condition_values.shape} for batch_size {batch_size}")
                if condition_values.shape[1] != self.config.slider_n_variables:
                    raise ValueError(f"Expected {self.config.slider_n_variables} slider variables per sample. Got {condition_values.shape[1]}")
                processed_condition_values = condition_values.to(device=device) # Ensure device consistency

        # =========================
        # Original GPT2Model Input Processing
        # ========================
        if token_type_ids is not None:
            token_type_ids = token_type_ids.view(-1, input_shape[-1])
        if position_ids is not None:
            position_ids = position_ids.view(-1, input_shape[-1])

        if past_key_values is None:
            past_length = 0
            past_key_values = tuple([None] * len(self.h))
        else:
            past_length = past_key_values[0][0].size(-2)

        if position_ids is None:
            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)
            position_ids = position_ids.unsqueeze(0) #.view(-1, input_shape[-1])

        # Attention mask processing (moved after potential input_ids processing)
        if attention_mask is not None:
             if batch_size <= 0:
                  raise ValueError("batch_size has to be defined and > 0")
             attention_mask = attention_mask.view(batch_size, -1)
             # HF's standard causal mask prep depends on attn implementation
             if self._attn_implementation == "flash_attention_2":
                 attention_mask = attention_mask if 0 in attention_mask else None # FA2 handles causal mask implicitly if None
             elif self._attn_implementation == "sdpa" and not output_attentions:
                 # Assuming causal mask needed if attention_mask is None or standard 1s/0s mask
                  attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
                      attention_mask,
                      (batch_size, input_shape[-1]),
                      inputs_embeds if inputs_embeds is not None else self.wte(input_ids), # Need embeds for dtype
                      past_length
                  )
             else: # Eager or SDPA with output_attentions
                 attention_mask = attention_mask[:, None, None, :] # Add head/query dims
                 attention_mask = attention_mask.to(dtype=self.dtype) # Ensure correct dtype
                 attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min


        # Prepare head mask if needed
        head_mask = self.get_head_mask(head_mask, self.config.n_layer)

        # Get Embeddings
        if inputs_embeds is None:
            inputs_embeds = self.wte(input_ids.view(-1, input_shape[-1]))
        position_embeds = self.wpe(position_ids)
        hidden_states = inputs_embeds + position_embeds

        if token_type_ids is not None:
            token_type_embeds = self.wte(token_type_ids)
            hidden_states = hidden_states + token_type_embeds

        hidden_states = self.drop(hidden_states)

        output_shape = input_shape + (hidden_states.size(-1),) # Correct output shape calc

        if self.gradient_checkpointing and self.training:
            if use_cache:
                logger.warning_once(
                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                )
                use_cache = False

        presents = () if use_cache else None
        all_self_attentions = () if output_attentions else None
        all_hidden_states = () if output_hidden_states else None

        for i, block in enumerate(self.h):
            layer_past = past_key_values[i] if past_key_values is not None else None

            # Model parallel handling (simplified, assuming block-level assignment)
            if self.model_parallel:
                 block_device = block.ln_1.weight.device # Infer block device
                 hidden_states = hidden_states.to(block_device)
                 if layer_past is not None:
                     layer_past = tuple(past_state.to(block_device) for past_state in layer_past)
                 if attention_mask is not None:
                     attention_mask = attention_mask.to(block_device)
                 # head_mask is prepared per layer, use head_mask[i]
                 if isinstance(head_mask, torch.Tensor):
                     layer_head_mask = head_mask[i].to(block_device)
                 else: # head_mask is None or List[None]
                     layer_head_mask = None
            else:
                 layer_head_mask = head_mask[i] if head_mask is not None else None


            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            if self.gradient_checkpointing and self.training:
                 # Wrapper for gradient checkpointing to handle condition_values
                 def create_custom_forward(module):
                     def custom_forward(*inputs):
                         # inputs should be: hidden_states, layer_past, attention_mask, head_mask
                         # We need to add condition_values from the outer scope
                         # Note: Checkpointing with use_cache=True behaviour is tricky, usually disabled
                         return module(inputs[0], # hidden_states
                                        layer_past=None, # Checkpointing usually incompatible with past
                                        attention_mask=inputs[1], # attention_mask
                                        head_mask=inputs[2], # head_mask
                                        use_cache=False, # Force use_cache=False inside checkpoint
                                        output_attentions=output_attentions,
                                        condition_values=processed_condition_values)
                     return custom_forward

                 outputs = torch.utils.checkpoint.checkpoint(
                     create_custom_forward(block),
                     hidden_states,
                     attention_mask,
                     layer_head_mask,
                     use_reentrant=False # Recommended for newer PyTorch versions
                 )
                 # Checkpoint outputs: hidden_states, (attentions) - no past
                 hidden_states = outputs[0]
                 if output_attentions:
                      all_self_attentions = all_self_attentions + (outputs[1],)

            else:
                 outputs = block(
                    hidden_states,
                    layer_past=layer_past,
                    attention_mask=attention_mask,
                    head_mask=layer_head_mask,
                    # encoder_hidden_states=encoder_hidden_states, # Pass if/when needed
                    # encoder_attention_mask=encoder_attention_mask, # Pass if/when needed
                    use_cache=use_cache,
                    output_attentions=output_attentions,
                    # Pass the processed slider variables
                    condition_values=processed_condition_values
                )

                 hidden_states = outputs[0]
                 if use_cache:
                    presents = presents + (outputs[1],)
                 if output_attentions:
                    # output index depends on use_cache
                    attention_output_idx = 2 if use_cache else 1
                    all_self_attentions = all_self_attentions + (outputs[attention_output_idx],)


            # Model Parallel: Move hidden_states to the next device if needed
            # This logic needs careful verification based on the exact device_map structure
            if self.model_parallel:
                 for k, v in self.device_map.items():
                     # Assuming v is list of layer indices for device k
                     if i == v[-1] and "cuda:" + str(k) != self.last_device:
                         next_device_idx = k + 1
                         # Find the next device that has layers assigned
                         while next_device_idx not in self.device_map and next_device_idx <= int(self.last_device.split(":")[1]):
                              next_device_idx += 1
                         if next_device_idx in self.device_map:
                              hidden_states = hidden_states.to("cuda:" + str(next_device_idx))
                         else: # Should reach last_device eventually
                              hidden_states = hidden_states.to(self.last_device)


        # Final Layer Norm
        # Ensure hidden_states is on the last device before final LN
        if self.model_parallel:
             hidden_states = hidden_states.to(self.last_device)
        hidden_states = self.ln_f(hidden_states)

        hidden_states = hidden_states.view(output_shape)
        # Add last hidden state
        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        if not return_dict:
            return tuple(
                v
                for v in [hidden_states, presents, all_hidden_states, all_self_attentions]
                if v is not None
            )

        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=presents,
            hidden_states=all_hidden_states,
            attentions=all_self_attentions,
        )


@add_start_docstrings(
    """
    The SliderGPT model transformer with a language modeling head on top.
    Can be conditioned on float vectors (`condition_values`). Handles missing
    condition values by masking their contribution in the attention mechanism.
    """,
    GPT2_START_DOCSTRING, # Use GPT2 docstring as base
)
class SliderGPT(SliderGPT2PreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]

    def __init__(self, config: SliderGPT2Config):
        super().__init__(config)
        # Use SliderGPT2Model as the transformer body
        self.transformer = SliderGPT2Model(config)
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)

        # Model parallel attributes
        self.model_parallel = False
        self.device_map = None

        # Initialize weights and apply final processing
        self.post_init()

    @add_start_docstrings(PARALLELIZE_DOCSTRING)
    def parallelize(self, device_map=None):
        warnings.warn(
            "`SliderGPT.parallelize` is deprecated and will be removed in v5 of Transformers, you should load"
            " your model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own"
            " `device_map` but it needs to be a dictionary module_name to device, so for instance {'transformer.h.0':"
            " 0, 'transformer.h.1': 1, ...}",
            FutureWarning,
        )
        # Determine device map based on transformer layers
        self.device_map = (
             get_device_map(len(self.transformer.h), range(torch.cuda.device_count()))
             if device_map is None
             else device_map
         )
        # Parallelize the transformer body using its own method
        self.transformer.parallelize(self.device_map)
        # Place LM head on the same device as the final transformer layer norm
        self.lm_head = self.lm_head.to(self.transformer.ln_f.weight.device)
        self.model_parallel = True


    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)
    def deparallelize(self):
        warnings.warn(
            "Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.",
            FutureWarning,
        )
        # Deparallelize the transformer body
        self.transformer.deparallelize()
        # Move LM head back to CPU
        self.lm_head = self.lm_head.to("cpu")
        self.model_parallel = False
        torch.cuda.empty_cache()

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):
        token_type_ids = kwargs.get("token_type_ids", None)
        # only last token for inputs_ids if past is defined in normal flag cases
        if past_key_values:
            input_ids = input_ids[:, -1].unsqueeze(-1)
            if token_type_ids is not None:
                token_type_ids = token_type_ids[:, -1].unsqueeze(-1)

        attention_mask = kwargs.get("attention_mask", None)
        position_ids = kwargs.get("position_ids", None)

        if attention_mask is not None and position_ids is None:
            # create position_ids on the fly for batch generation
            position_ids = attention_mask.long().cumsum(-1) - 1
            position_ids.masked_fill_(attention_mask == 0, 1)
            if past_key_values:
                position_ids = position_ids[:, -1].unsqueeze(-1)

        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step
        if inputs_embeds is not None and past_key_values is None:
            model_inputs = {"inputs_embeds": inputs_embeds}
        else:
            model_inputs = {"input_ids": input_ids}

        # Add condition_values to model inputs if provided
        condition_values = kwargs.get("condition_values", None)
        if condition_values is not None:
             # Check if slider is enabled in config
             if not getattr(self.config, "slider_on", False):
                  logger.warning_once("condition_values provided to generation but config.slider_on=False. Ignoring condition_values.")
             else:
                  model_inputs["condition_values"] = condition_values

        model_inputs.update(
            {
                "past_key_values": past_key_values,
                "use_cache": kwargs.get("use_cache"),
                "position_ids": position_ids,
                "attention_mask": attention_mask,
                "token_type_ids": token_type_ids,
            }
        )
        return model_inputs

    # @add_code_sample_docstrings(
    #     checkpoint=_CHECKPOINT_FOR_DOC, # Use appropriate checkpoint name
    #     output_type=CausalLMOutputWithPast,
    #     config_class=_CONFIG_FOR_DOC, # Use SliderGPT2Config
    # )
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
        attention_mask: Optional[torch.FloatTensor] = None,
        token_type_ids: Optional[torch.LongTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        head_mask: Optional[torch.FloatTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        encoder_hidden_states: Optional[torch.Tensor] = None, # Keep for compatibility if needed
        encoder_attention_mask: Optional[torch.FloatTensor] = None, # Keep for compatibility
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        # slider variables
        condition_values: Optional[torch.Tensor] = None,
    ) -> Union[Tuple, CausalLMOutputWithPast]:
        r"""
        Args:
            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
                `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`
                are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
            condition_values (`torch.FloatTensor` of shape `(batch_size, config.slider_n_variables)`, *optional*):
                Float values to condition the generation. If `config.slider_on` is True and this is `None`,
                generation will proceed as if all condition values are missing. Can contain `MISSING_CONDITION_VALUE`
                (e.g., -100.0) for individual missing conditions within the tensor.
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        transformer_outputs = self.transformer(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_attention_mask,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            # Pass condition_values down to the base model
            condition_values=condition_values,
        )
        hidden_states = transformer_outputs[0]

        # Ensure hidden_states are on the correct device before LM head
        if self.model_parallel:
             # LM head is on the same device as the final layer norm
             lm_head_device = self.lm_head.weight.device
             if hidden_states.device != lm_head_device:
                  hidden_states = hidden_states.to(lm_head_device)

        lm_logits = self.lm_head(hidden_states)

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = lm_logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()

            # Check shapes after shift (handle potential off-by-one errors)
            if shift_logits.shape[1] != shift_labels.shape[1]:
                 # This might happen if labels have different length than input_ids/logits
                 # Adjust based on the shorter sequence length after shifting
                 min_len = min(shift_logits.shape[1], shift_labels.shape[1])
                 shift_logits = shift_logits[:, :min_len, :]
                 shift_labels = shift_labels[:, :min_len]
                 # Optionally add a warning here if lengths didn't match
                 # logger.warning(f"Logit and label lengths mismatched after shift. Truncating to {min_len}.")


            # Flatten the tokens
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

        if not return_dict:
            output = (lm_logits,) + transformer_outputs[1:]
            return ((loss,) + output) if loss is not None else output

        return CausalLMOutputWithPast(
            loss=loss,
            logits=lm_logits,
            past_key_values=transformer_outputs.past_key_values,
            hidden_states=transformer_outputs.hidden_states,
            attentions=transformer_outputs.attentions,
        )


    @staticmethod
    def _reorder_cache(
        past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor
    ) -> Tuple[Tuple[torch.Tensor]]:
        """
        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or
        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct
        beam_idx at every generation step.
        """
        return tuple(
            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)
            for layer_past in past_key_values
        )
